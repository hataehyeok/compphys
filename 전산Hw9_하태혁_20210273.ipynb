{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO8kNtTbyk4K"
      },
      "source": [
        "# Problem 1 (Practice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_tCMo2oynCC"
      },
      "source": [
        "We learned how to construct an AND operator in the lecture. We will repeat this exercise, this time constructing an OR operator.\n",
        "\n",
        "(a) Taking the two inputs as $x_1$ and $x_2$, predict what the values of the fitting parameters $W_1, W_2,$ and $b$ should be, where $v = W_1x_1 + W_2x_2 + b$. (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$W_1 = 1, W_2 = 1, b = -0.5$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJa0aLWU65dv"
      },
      "source": [
        "(b) Write a program that learns to make the OR operator using perceptrons. Use $Nep=1000$ epochs, and a learning rate of $c=0.5$, and the sigmoid activation function\n",
        "\\begin{align*}\n",
        "  y=h(v)=\\frac{1}{1+e^{-v}}.\n",
        "\\end{align*}\n",
        "Use the gradient descent method as the learning algorithm. Use the initial guesses $W_1 = 1, W_2=0.1, b=-1$. For every epochs, save the squared error, $\\mathbf{W}$, and $b$. Plot the squared error as a function of epoch. Print out the initial and final $\\mathbf{W}$ vector and bias $b$. Do the final values match your predictions? [Hint: Normalize $\\mathbf{W}$ and $b$ by $W_1$ at when printing them.] (40 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 4\n",
        "Nep = 1000\n",
        "c = 0.5\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x_train = np.array([[1,1], [1,0], [0,1], [0,0]])\n",
        "a_train = np.array([1, 1, 1, 0])\n",
        "\n",
        "e = np.zeros(4)\n",
        "W = np.array([1,0.1])\n",
        "b = -1\n",
        "\n",
        "xarr = []\n",
        "yarr = []\n",
        "Warr = []\n",
        "barr = []\n",
        "print(W/W[0], b/W[0])\n",
        "\n",
        "for ep in range(Nep):\n",
        "    for n in range(N):\n",
        "        x = x_train[n]\n",
        "        a = a_train[n]\n",
        "        v = np.sum(W * x) + b\n",
        "        y = sigmoid(v)\n",
        "        e = a - y\n",
        "        W = W + c*e*y*(1-y)*x\n",
        "        b = b + c*e*y*(1-y)\n",
        "    xarr.append(ep)\n",
        "    yarr.append(e*e)\n",
        "    Warr.append(W)\n",
        "    barr.append(b)\n",
        "\n",
        "print(W/W[0], b/W[0])\n",
        "plt.plot(xarr, yarr)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxrSY2af8Hz7"
      },
      "source": [
        "(c) Generate 1000 random $(x_1,x_2)$ points from (0,0) to (1,1). Make a scatter plot of these points, with the colors representing the value of $h(v)$. Are the points well-classified? (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xlist = []\n",
        "ylist = []\n",
        "\n",
        "for i in range(1000):\n",
        "    x = np.random.random(2)\n",
        "    y = np.sum(W@x) + b\n",
        "    xlist.append(x)\n",
        "    ylist.append(sigmoid(y))\n",
        "\n",
        "colors = plt.cm.RdBu(ylist)\n",
        "xlist = np.array(xlist)\n",
        "plt.scatter(xlist[:,0], xlist[:,1], marker='o',c=colors)\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.xlim(0,1)\n",
        "plt.ylim(0,1)\n",
        "ax = plt.gca()\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S_SzqJn-gN8"
      },
      "source": [
        "(d) Make a plot of the separator line $v=0$ as a function of epochs. (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1 = np.linspace(0, 1, 100)\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(Warr)))\n",
        "for n in range(len(barr)):\n",
        "    plt.plot(x1, -(Warr[n][0]*x1+barr[n])/Warr[n][1], c=colors[n])\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "ax = plt.gca()\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD4FDx0VFECQ"
      },
      "source": [
        "(e) The bias $b$ can actually by written as $W_0$, and then $v=\\sum_{n=0}^2 W_n x_n$ where $x_0=1$. Modify your code so that instead of $b$, you provide a training data set including $x_0=1$ and learn the weight vector with three elements. Print out the final weight vector. Do the values agree with your previous result? (20 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = np.array([[1,1,1], [1,1,0], [1,0,1], [1,0,0]])\n",
        "a_train = np.array([1, 1, 1 ,0])\n",
        "e = np.zeros(4)\n",
        "W = np.array([-1,1,0.1])\n",
        "xarr = []\n",
        "yarr = []\n",
        "Warr = []\n",
        "print(W/W[1])\n",
        "for ep in range(Nep):\n",
        "    for n in range(N):\n",
        "        x = x_train[n]\n",
        "        a = a_train[n]\n",
        "        v = np.sum(W*x)\n",
        "        y = sigmoid(v)\n",
        "        e = a - y\n",
        "        W = W + c*e*y*(1-y)*x\n",
        "        b = b + c*e*y*(1-y)\n",
        "    xarr.append(ep)\n",
        "    yarr.append(e*e)\n",
        "    Warr.append(W)\n",
        "\n",
        "print(W/W[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5mJp-MNW1ya"
      },
      "source": [
        "---\n",
        "# Problem 2\n",
        "\n",
        "In the lecture, the updates for the weight vector elements were shown according to the gradient descent method, namely\n",
        "\\begin{align*}\n",
        "    \\Delta W_{ij} &= - c' \\frac{\\partial E}{\\partial W_{ij}} = c e_i y_i (1-y_i)x_j,\\\\\n",
        "    \\Delta b_i &= -c' \\frac{\\partial E}{\\partial b_i}=c e_i y_i(1-y_i),\n",
        "\\end{align*}\n",
        "where $c'$ is the learning rate.\n",
        "\n",
        "Use the error function\n",
        "\\begin{align*}\n",
        "    E= \\sum_i (a_i-y_i)^2,\n",
        "\\end{align*}\n",
        "where $a_i$ is the training data, to derive the updates for the sigmoid activation function\n",
        "\\begin{align*}\n",
        "    v_i &= \\sum_j W_{ij}x_j + b_i,\\\\\n",
        "    y_i &= h(v_i).\n",
        "\\end{align*}\n",
        "\n",
        "(30 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFC_OHULW1ya"
      },
      "source": [
        "---\n",
        "# Problem 3\n",
        "\n",
        "In this problem, we will see that the gradient descent method is not the only learning algorithm, of which there can be various types.\n",
        "\n",
        "Consider the activation function\n",
        "\\begin{align*}\n",
        "    y = h(v) = \\text{sign}\\left({\\sum_i W_{i}x_i }\\right).\n",
        "\\end{align*}\n",
        "This is similar but different from the Heaviside step function in that False is represented by -1 instead of 0.\n",
        "\n",
        "Instead of the gradient descent method, we will use the following algorithm:\n",
        "1.  Identify a misclassified point $y_n$ in the training data, i.e.,$\\text{sign}\\left(\\sum_i W_{i}x_{in}\\right) = y_n = -a_n$ where $a_n$ is the training output.\n",
        "\n",
        "2.  Update the weight vector so that $W_{i} \\rightarrow W_{i} + a_n x_{in}$\n",
        "\n",
        "3.  Repeat until all training points are classified correctly.\n",
        "\n",
        "(a) Explain why this algorithm would update the weight vector toward the correct value. [Hint: If a misclassified $y_n=1$, that means $a_n=-1$ and so the change in $W_i$ should act in such a way that decreases the argument of $h(v)$. Similarly for a misclassified $y_n=-1$.] (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBX7rpNEW1ya"
      },
      "source": [
        "(b) Implement this algorithm for the operator for the initial weight vector $\\mathbf{W}=(-2,1,0.2)$. What is the final weight? How many epochs did it take? (40 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoEZLas5W1yb"
      },
      "source": [
        "(c) Repeat Problem 1(c) for the obtained weight. Are the training points classified correctly? [Hint: there are an infinite number of lines that can separate the four points according to the OR operator output] (10 pts)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "comp_phys",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
